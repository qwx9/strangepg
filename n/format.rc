#!/bin/rc
# FIXME: untested, not portable; also need a plaintext version
# convert GFAv1 graph into binary edge list sorted by node degree
# format:
#	nodes[] edges[] degrees[] nnodes[8] nedges[8] ndegrees[4]
#	nodes: { offset[8] }[nnodes]
#		relative offset from start of edge array
#	edges: { index[8] }[nedges]
#		index into nodes array
#	degrees: { degree[4] index[8] }
#		index into edges array
# 1) filter only link records, output mirrored edges and sort by id
#	select node columns: excepting maybe cut(1), awk here is fastest
#	and without memory overhead;
#	sort by id: sort(1) is fast and without memory overhead
#	result: edge list of the form:	u_id \t v_id
# note: unix sort(1) is significantly slower than plan9's??
# note: using bioawk because of plan9 limitations; gawk would work fine
# note: nodes are reduced to points; sequence, orientation, direction
#	are considered metadata and left out
# 2) count degree for each node
#	essentially, uniq -c | sort -n, except we must restrict the count
#	to only the first field; so use awk instead again only retaining
#	a minimal amount into memory: count lines with same node and
#	concatenate them all in the form: degree u_id v_id[], then pass
#	again to sort(1)
# note: if ever we use the uniq -c method, we must specify C locale
#	for both commands
# note: technically, the second half of the file is redundant
# 3) binarize the file
#	two methods: use the previously obtained file in external memory
#	by looking up node id's with binary searches, or load the whole
#	thing into memory, mapping string id's to a u64 index and an
#	edge array; here only the second is implemented for now; result
#   is the file formatted as specified above: nodes are fixed size
#	records and their id is implicit; edges are a linear array of
#	nodes indices ordered by degree and by node; node records point
#	to the offset of the start of their corresponding adjacencies
#	in the edge array; we also keep track of where nodes of a given
#	degree start; necessary metadata to parse the file is appended
#	at the end
# later: separately index metadata in the gfa file: associate each
# node/edge record with an offset into the gfa to parse on-demand;
# below loads everything into memory, will not scale beyond 1e6-1e7 nodes
# binarization not doable with awk; would have to write it in c anyway
bioawk '$1 == "L"{ print $2, $4 "\n" $4, $2 }' $* \
	| sort -ud \
	| bioawk '
{
	if(u != $1){
		if(NR > 0)
			print d, line
		line = u = $1
		d = 0
	}
	line = line OFS $2
	d++
}
END{
	if(NR > 0)
		print d, line
}
' \
	| sort -n \
	| python -c '
import fileinput
from os import write as outb
from struct import pack
id = {}
edges = []
nr = 1L
ne = 0L
off = 0L
D = {}
od = 0
for l in fileinput.input():
	s = l.rstrip("\n").split()
	d = int(s[0])
	for i in range(2, 2 + d):
		edges.append(s[i])
	ne += d
	if od != d:
		D[d] = nr
		od = d
	id[s[1]] = nr
	nr += 1
	outb(1, pack("Q", off))
	off += d * 8
for e in edges:
	outb(1, pack("Q", id[e]))
for i in D.keys():
	outb(1, pack("I", i))
	outb(1, pack("Q", D[i]))
outb(1, pack("Q", nr))
outb(1, pack("Q", ne))
outb(1, pack("I", len(D.keys())))
'
